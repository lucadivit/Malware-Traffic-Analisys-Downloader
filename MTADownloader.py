from bs4 import BeautifulSoup
import sys, os, random, urllib3, getopt, requests, time

urlMTA = 'https://www.malware-traffic-analysis.net/'
numberOfPcapToDownload = 4 #Se 0 o se maggiore di quelli presenti (1566) si scarica tutto l'archivio
randomly = 1 #scarica pcap random o prende i primi X (a partire dal 2013)
saveLink = "" #Salva i link per il Download in un file <nome>.txt
folderName = "Pcaps"

def setFolderName(name):
    global folderName
    folderName = name

def getFolderName():
    return folderName

def setNumberOfPcapToDownload(numOfPcap):
    global numberOfPcapToDownload
    numberOfPcapToDownload = numOfPcap
    checkArgument()

def setRandomly(value):
    global randomly
    randomly = value
    checkArgument()
    
def setSaveLink(value):
    global saveLink
    saveLink = value
    checkArgument()
    
def getURLMTA():
    return urlMTA

def getHtml(urlToParse):
    try:
        http_r = urllib3.PoolManager()
        html = http_r.request("GET", urlToParse).data
        html = html.decode("utf8")
        bf = BeautifulSoup(html)
        return bf
    except:
        print("Errore in getHtml()")
        sys.exit()

def getAllLinkToAnnualArchive(arrayOfLinks, year=None):
    linkToAnnualArchive = []
    if(year is not None):
        year = str(year) + "/"
    for link in arrayOfLinks:
        try:
            int(link[:4])
            converted = True
        except:
            converted = False
        if (converted):
            if(year is not None):
                if(year in link):
                    linkToAnnualArchive.append(urlMTA + link)
            else:
                linkToAnnualArchive.append(urlMTA + link)
    return linkToAnnualArchive

def getAllLinkInMainPage(mainPage):
    generalLinks = []
    index = 0
    for li in mainPage.findAll("li"):
        for a in li.findAll("a"):
            generalLinks.insert(index, a["href"])
            index = index + 1
    return generalLinks

def getLinkToPcap(linkToYears):
    linkToPcap = []
    index = 0
    for link in linkToYears:
        annualPage = getHtml(link)
        for ul in annualPage.findAll("ul"):
            for li in ul.findAll("li"):
                for a in li.findAll("a", {"class": "list_header"}):
                    head, step, tail = link.partition("/index")
                    linkHead = head + "/"
                    linkToPcap.insert(index, linkHead + a["href"])
                    index = index + 1
    return linkToPcap

def getLinkForDownload(linksToPcap):
    numberOfPcapInSite = len(linksToPcap)
    arrayOfRandomNumbers = []
    arrayOfLinkForDownload = []
    if(numberOfPcapToDownload > 0 and numberOfPcapToDownload <= numberOfPcapInSite):
        if(randomly):
            pcapTrovato = False
            index = 0
            while(len(arrayOfLinkForDownload) != numberOfPcapToDownload):
                rnd = random.randint(0, numberOfPcapInSite)
                html = getHtml(linksToPcap[rnd])
                for a in html.findAll("a", {"class": "menu_link"}, href=True):
                    pcapTrovato = False
                    href = a["href"]
                    if(href.find("pcap") != -1 or  href.find("pcaps") != -1):
                        head, step, tail = linksToPcap[rnd].partition("/index")
                        link = head + "/" + href
                        link.replace(" ", "")
                        if link in arrayOfLinkForDownload:
                            pass
                        else:
                            pcapTrovato = True
                            arrayOfLinkForDownload.insert(index, link)
                            break
                    else:
                        pass
                if(not pcapTrovato):
                    print("La pagina " + linksToPcap[index] + " non contiene .pcap. Saltata." + "\n")
                else:
                    pass
                index = index + 1
        else:
            pcapTrovato = False
            index = 0
            while(len(arrayOfLinkForDownload) != numberOfPcapToDownload):
                html = getHtml(linksToPcap[index])
                for a in html.findAll("a", {"class": "menu_link"}, href=True):
                    pcapTrovato = False
                    href = a["href"]
                    if(href.find("pcap") != -1 or  href.find("pcaps") != -1):
                        head, step, tail = linksToPcap[index].partition("/index")
                        link = head + "/" + href
                        link.replace(" ", "")
                        arrayOfLinkForDownload.insert(index, link)
                        pcapTrovato = True
                        break
                    else:
                        pass
                if(not pcapTrovato):
                    print("La pagina " + linksToPcap[index] + " non contiene .pcap. Saltata." + "\n")
                else:
                    pass
                index = index + 1
    elif (numberOfPcapToDownload == 0 or numberOfPcapToDownload > numberOfPcapInSite):
        for i in range(0, numberOfPcapInSite):
            pcapTrovato = False
            html = getHtml(linksToPcap[i])
            for a in html.findAll("a", {"class": "menu_link"}, href=True):
                pcapTrovato = False
                href = a["href"]
                if(href.find("pcap") != -1 or  href.find("pcaps") != -1):
                    head, step, tail = linksToPcap[i].partition("/index")
                    link = head + "/" + href
                    link.replace(" ", "")
                    arrayOfLinkForDownload.insert(i, link)
                    pcapTrovato = True
                    break
                else:
                    pass
            if(not pcapTrovato):
                print("La pagina " + linksToPcap[i] + " non contiene .pcap. Saltata." + "\n")
            else:
                pass
    else:
        print("Errore generico. Programma terminato" + "\n")
        sys.exit()
    if(saveLink != ""):
        writeLinkInFile(arrayOfLinkForDownload)
    return arrayOfLinkForDownload
    
def checkURL(urlToValidate):
    i = 0
    urlValid = False
    while True:
        try:
            page = requests.get(urlToValidate)
            urlValid = True
        except:
            i+=1
            if(i == 5):
                break
            else:
                print("\nConnection refused by the server...\n")
                print("\nLet me sleep for 5 seconds\n")
                time.sleep(5)
                print("\nRetrying...\n")
                continue
        break
    return urlValid

def repaireName(name):
    if(not checkName(name)):
        try:
            os.rename(name, name[:-1])
            return True
        except:
            return False
    else:
        return True

def checkName(name):
    i = len(name) - 1
    if( name[i] != "p"):
        return False
    else:
        return True

def checkIfFileIsAlreadyDownloaded(nameOfZip):
    downloaded = False
    if(not checkName(nameOfZip)):
        if(os.path.isfile(nameOfZip[:-1])):
            downloaded = True
        else:
            downloaded = False
    else:
        if(os.path.isfile(nameOfZip)):
            downloaded = True
        else:
            downloaded = False
    return downloaded

def downloadPcap(urlToDownload):
    downloaded = False
    os.chdir(os.getcwd())
    createFolder(folderName)
    os.chdir(os.getcwd() + "/" + folderName + "/")
    nameOfZip = os.path.basename(urlToDownload)
    if(not checkIfFileIsAlreadyDownloaded(nameOfZip)):
        if(checkURL(urlToDownload)):
            print("Sto scaricando " + nameOfZip)
            try:
                http_r = urllib3.PoolManager()
                pcapFile = http_r.request("GET", urlToDownload, preload_content=False)
                with open(nameOfZip, "wb") as local_file:
                    local_file.write(pcapFile.read())
                    if(repaireName(nameOfZip)):
                        downloaded = True
                        print("Download Completato" + "\n")
                    else:
                        pass
            except:
                print("Errore durante il download" + "\n")
                downloaded = False
        else:
            print(urlToDownload + ": URL non valido" + "\n")
            downloaded = False
    else:
        print(nameOfZip + " gia presente! Non sara scaricato" + "\n")
        downloaded = True
    os.chdir("..")
    return downloaded

def createFolder(folderName):
    os.chdir(os.getcwd())
    if not os.path.exists(folderName):
        os.makedirs(folderName)

def checkArgument():
    if(randomly != 1 and randomly != 0):
        print("Inserire un valore valido per l'attributo randomly" + "\n")
        sys.exit()
    else: 
        pass
    if(numberOfPcapToDownload < 0):
        print("Inserire un valore valido per l'attributo pcap_number" + "\n")
        sys.exit()
    else:
        pass
    if(not isinstance(saveLink, str)):
        print("Inserire un valore valido per l'attributo save_link" + "\n")
    return

def writeLinkInFile(linkToWrite):
    writer = open(saveLink + ".txt", 'w')
    for link in linkToWrite:
        writer.write(link)
    writer.close()
    return

def main(argv):
    global randomly
    global numberOfPcapToDownload
    year = None
    if(len(argv) == 1 and argv[0] == "--help"):
        helpFile = open("helpMTA.txt")
        for line in helpFile:
            print (line)
        sys.exit(2)
    try:
        opts, argv = getopt.getopt(argv, "", ("pcap_number=", "randomly=", "save_link=", "output_folder=", "year="))
    except getopt.GetoptError as err:
        print(str(err))
        sys.exit(2)
    for opt, arg in opts:
        if(opt == "--pcap_number"):
            setNumberOfPcapToDownload(int(arg))
        elif(opt == "--randomly"):
            setRandomly(int(arg))
        elif(opt == "--save_link"):
            setSaveLink(arg)
        elif(opt == "--output_folder"):
            setFolderName(arg)
        elif(opt == '--year'):
            year = arg
    htmlBF = getHtml(urlMTA)
    generalLinks = getAllLinkInMainPage(htmlBF)
    linkToAnnualPages = getAllLinkToAnnualArchive(generalLinks, year)
    linksToPcap = getLinkToPcap(linkToAnnualPages)
    linkForDownload = getLinkForDownload(linksToPcap)
    for link in linkForDownload:
        downloadPcap(link)

if __name__ == '__main__':
    main(sys.argv[1:])